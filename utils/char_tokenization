
class CharacterTokenization():
    def __init__(self, symbols = SYMBOLS):
        self.symbols_len = len(symbols)

        self.symbol_to_idx = {
            s: i for i, s in enumerate(symbols)
        }

        self.idx_to_symbol = {v: k for k, v in self.symbol_to_idx.items()}

    def text_to_seq_char_level(self, text):
        text = text.lower()
        seq = []
        for symbol in text:
            idx = self.symbol_to_idx.get(symbol, None)
            if idx is not None:
                seq.append(idx)
        seq.append(self.symbol_to_idx.get(EOS))
        return torch.IntTensor(seq)

    def seq_to_text(self, seq, remove_pad = True):
        text = ""
        for idx in seq:
            symbol = self.idx_to_symbol.get(idx.item(), None)
            if remove_pad and symbol == PAD:
                symbol = ""
            text += symbol
        return text


